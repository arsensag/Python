Допустим, есть кравлер https://instagram.com, который должен грузить вообще все сообщения этой соц сети
(Включая комментарии). Опишите возможные стратегии загрузки и рассчитайте в соответствии с ними и ограничениями
апи необходимое количество(можно приблизительное) ресурсов(приложений, выделенных ip, места для записи 
данных за 1 день в среднем) для стабильной загрузки. 


Стратегии:
    
    1. Обходом графа (по подпискам и подписчикам), в таком случае требуется хранить множество пройденных
    пользователей, хранить в дереве. Возникает проблема с компонентами связности, трудно выйти из нее.
    
    2. Делая запрос в поиск, в лексикографическом порядке. Например (aaaaab,aaaaac,...zzzzzz). Таким образом 
    не потребуется множество уже имеющихся пользователей. Нет проблемы с компонентами
    
    
Подсчеты:
     
     Инстаграммом разрешено делать 5000 запросов в час.
     
     1. Запрос на информацию о пользователе + Запрос постов (recent) + запрос комментов на каждый пост + запрос подписок + запрос подписчиков.
     
     
     
     
     2. Запрос на поиск +  Запрос постов (recent)  + запрос на комменты
     
    
    Пусть у каждого пользователя по 300 постов. Тогда:
    
    1 вариант:
        1 запрос на информацию о пользователе, чтобы узнать количество постов (300).
        1 запрос на все посты
        300 запросов на комменты к каждому посту
        2 запроса на подписки и подписчики, которых добавляем в очередь
        
        Итого за 5000 запросов = 16,5 пользователей, т.е. примерно 4950 постов в час.
        Каждую фотографию будем считать равно 200KB. 4950*200KB=966MB в час с 1 потока.
        
        
     2 вариант:
        1 Запрос на поиск на 1000 пользователей например на отдельной машине собирающей пользователей.
        Тогда:
        1 запрос на все посты
        300 запросов на комменты к каждому посту
        
        Итого за 5000 запросов = 16,6 пользователей, т.е. 4980 постов в час.
        Не особо отличается от предыдущего случая. Достаточно 1гб в час с 1 потока работа кравлера.
        
    Чем больше потоков - тем лучше очевидно. Но второй способ здесь имеет преимущество в масштабируемости, так как не возникает
    проблемы конкуренции за ресурсы между потоками. С другой стороны, 16 записей в множество в течении часа (1 случай) это не много.
    
    
     
